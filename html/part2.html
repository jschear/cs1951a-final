<html>

<head>
    <link rel="stylesheet" href="http://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Final Project</title>
</head>

<body>

    <div class="row">
        <div class="col-md-2"></div>
        <div class="col-md-8">

            <div class="page-header center">
                <h1>Restaurant Classification <br><small>Elijah ben Izzy and Jonathan Schear (ebenizzy and jschear)</small></h1>
            </div>

            <div>

            <h2> Problem </h2>
            <p>In this study, we attempted to predict the category (specifically the cuisine) of restaurants in the dataset from the text of restaurant review and the name of the restaurant with high accuracy. We only attempted to predict a certain subset of business, those which were marked as "Restaurants" and whose other category labels contained cuisine types types. In all, there were 120 different classifications, ranging from American (New) and American (Traditional) to Dim Sum, Brazilian, Chicken Wings, Vegan, and Soul Food. We chose to do this because it represented an interesting challenge. It was both a contained problem, as cuisines will often have different key indicator words, and a difficult one, which involved multi-class training and multilabel classification.</p>

            <h2>Hypothesis</h2>

            <p>We predict that for most restaurants (roughly 80%) on Yelp, their categories can be fairly accurately predicted by the text of their reviews and the name of the restaurants.</p>

            <h2> Methodology </h2>
            <h3> Classification Fields </h3>
            <p> We chose primarily to use the name of the restaurant and the text of the review as features, as they seemed to be the most suggestive of the cuisine of the restaurant. Choosing other fields such as price range and star rating would be making hard-to-justify assumptions about correlation of the data. Presuming that the cuisines correlate to review text is much more reasonable.</p>
            <h3> Classifier Selection </h3>

            <p> To solve this problem, we utilized the machine-learning package <code>sklearn</code>, a python library that excels at creating classifiers. To perform multi-label classification we decided to use the One-Vs-Rest <a href = http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier> classifier </a>, a clssifier that implements the one-vs rest algorithm for any given classifier. The one versus rest algorithm treats each class as an independent binary classifier, training the same number of classifiers as we have classes. When attempting to classify, the one versus all classifier will predict the chances of those features being classified to every category, and pick the labels that are above a certain reasonable threshold. We chose the One-Vs-Rest classifier over One-vs-One, as it is widely used and much more time-efficient() <code> O(n_classes) </code> as opposed to <code> O(n_classes<sup>2</sup>) </code> The sklearn implementation of One-Vs-Rest takes in any classifier as a parameter, letting us choose which one to use.
            </p>

            <p>We tried a variety of classifiers at first. Among the ones we could get to work were the Linear Support Vector Classifier, Gaussian Naive Bayes, and Logistic Regression. Furthermore, we attempted a variety of other ones, including Random Forest Classifiers, Decision Tree Classifiers, and Pipeline classifiers. We had difficulty getting these to produce coherent results. The random forest classifier would often break on a segmentation fault or obliterate the computers memory, rendering the entire machine inoperable (we were running the classifiers on a macbook air with <code>8gb</code> of ram)  Finally, we created our own classifier, that implemented a simple voting algorithm. The voting algorithm classifier took the results of our three top-performing classifiers, (LinearSVC, Gaussian Naive Bayes, and Logistic Regression), and merged the results to choose the best ones. Specifically, it took the average length of the returned results (as different classifiers might return different numbers of labels of labels), and outputted that many of the most common results. We found that the voting algorithm was outperformed by logistic regression, yet those two did significantly better than the others. </p>


            <h3> Data Selection </h3>
               <p>  We only chose to examine cuisines with more than 100 reviews corresponding to them -- this left out the following:

                <ul>
                <li> Argentine </li>
                <li> Burmese </li>
                <li> Cafeteria </li>
                <li> Cantonese </li>
                <li> Indonesian </li>
                <li> Kosher </li>
                <li> Colombian </li>
                <li> Live/Raw Food </li>
                <li> Russian </li>
                </ul>

                We felt comfortable doing this as these classes did not have enough data to justify classification.
                </p>
                <p>  <pre>        precision    recall  f1-score   support<br><br>     Afghan       0.85      0.38      0.52        29<br>    African       0.98      0.87      0.92        55<br>American (New)       0.90      0.82      0.86      7048<br>American (Traditional)       0.84      0.73      0.78      5531<br>Asian Fusion       0.89      0.78      0.83      1335<br>   Barbeque       0.95      0.87      0.91      1113<br>     Basque       1.00      0.37      0.54        30<br> Brasseries       1.00      0.83      0.91        30<br>  Brazilian       0.97      0.92      0.94        62<br>Breakfast & Brunch       0.87      0.80      0.83      4596<br>    British       0.99      0.98      0.98       319<br>    Buffets       0.92      0.83      0.88       922<br>    Burgers       0.91      0.84      0.87      2948<br>      Cafes       0.84      0.67      0.75       980<br>Cajun/Creole       0.93      0.80      0.86       174<br>  Cambodian       1.00      0.97      0.98        32<br>  Caribbean       0.95      0.78      0.86       322<br>Cheesesteaks       0.87      0.72      0.79       200<br>Chicken Wings       0.77      0.65      0.70       368<br>    Chinese       0.94      0.88      0.91      2303<br>Comfort Food       0.85      0.51      0.63        89<br>  Creperies       0.99      0.93      0.96       114<br>      Cuban       1.00      0.87      0.93        99<br>      Delis       0.87      0.78      0.82      1068<br>    Dim Sum       0.95      0.73      0.82       132<br>     Diners       0.82      0.58      0.68       466<br>  Ethiopian       0.97      0.91      0.94        67<br>  Fast Food       0.80      0.66      0.72      1382<br>   Filipino       0.98      0.70      0.82        61<br>Fish & Chips       0.94      0.78      0.85       117<br>     Fondue       0.97      0.93      0.95        95<br> Food Court       0.86      0.40      0.55        30<br>Food Stands       1.00      0.43      0.60        28<br>     French       0.97      0.86      0.91       478<br> Gastropubs       0.92      0.78      0.85       351<br>     German       1.00      0.90      0.95       127<br>Gluten-Free       0.88      0.74      0.80       609<br>      Greek       0.92      0.84      0.88       762<br>      Halal       0.88      0.65      0.75       100<br>   Hawaiian       0.96      0.89      0.92       366<br>   Hot Dogs       0.88      0.75      0.81       608<br>     Indian       0.99      0.96      0.97       481<br>      Irish       0.98      0.91      0.94       358<br>    Italian       0.93      0.89      0.91      4918<br>   Japanese       0.93      0.89      0.91      1803<br>     Korean       0.92      0.75      0.83       241<br>Latin American       0.91      0.87      0.89       564<br>   Lebanese       1.00      0.66      0.79        47<br>Mediterranean       0.91      0.82      0.86      1581<br>    Mexican       0.96      0.93      0.95      5837<br>Middle Eastern       0.86      0.75      0.80       415<br>Modern European       1.00      0.80      0.89        51<br>  Mongolian       0.86      0.90      0.88       113<br>   Moroccan       1.00      0.91      0.95        23<br>  Pakistani       0.86      0.73      0.79       157<br>Persian/Iranian       0.91      0.83      0.87        83<br>   Peruvian       0.96      0.83      0.89        29<br>      Pizza       0.95      0.92      0.94      4871<br>     Polish       1.00      0.86      0.93        74<br>      Salad       0.76      0.56      0.64       966<br> Sandwiches       0.83      0.73      0.78      4435<br>Scandinavian       1.00      0.83      0.91        66<br>    Seafood       0.89      0.78      0.83      1411<br>  Soul Food       0.98      0.89      0.93       302<br>       Soup       0.99      0.87      0.92       172<br>   Southern       0.97      0.89      0.93       346<br>    Spanish       1.00      0.71      0.83       103<br>Steakhouses       0.95      0.89      0.92      1695<br> Sushi Bars       0.95      0.94      0.94      2190<br>   Szechuan       0.69      0.71      0.70       127<br>  Taiwanese       0.91      0.84      0.88        38<br> Tapas Bars       1.00      0.40      0.57        43<br>Tapas/Small Plates       0.99      0.85      0.91       170<br>    Tex-Mex       0.84      0.73      0.78       548<br>       Thai       0.95      0.94      0.95      1253<br>    Turkish       1.00      0.57      0.73        35<br>      Vegan       0.84      0.69      0.76       592<br> Vegetarian       0.87      0.79      0.83      1365<br> Vietnamese       0.97      0.92      0.94       889<br><br>avg / total       0.90      0.83      0.86     73938 </pre> </p>
            <h3> Data Partition </h3>
            <p> 

            We chose to place <code>80%</code> of the data into training data, and the other <code>20%</code> into test data. We randomly partitioned it (using a consistent seed, as to be able to compare results). The results of that were no different than when we took the first <code>80%</code> for training and the last <code>20%</code> for test. 
            <p>

            We chose to place <code>80%</code> of the data into training data, and the other <code>20%</code> into test data. We randomly partitioned it (using a consistent seed, as to be able to compare results). The results of that were no different than when we took the first <code>80%</code> for training and the last <code>20%</code> for test.

            </p>

            <h2>Results</h2>

                <h3>Predictions</h3>

                <h3>Evaluations</h3>

                <h3>Visualizations</h3>
                <p>For each category, we pulled out the top 20 most discerning features. This helped us confirm that our classifier was making logical decisions (although these are features after stemming, so some are difficult understand). We also modified our stopwords list after examining some important words which we believed shouldn't be included as features.</p>
                <p>The following word clouds visualize five categories we found particularly interesting. Larger </p>
                <p>To view the complete set of word clouds, <a href="full_word_clouds.html">visit this page</a></p>
                <div id='clouds'></div>

            <p>

            </p>

            </div>

        <div class="col-md-2"></div>
    </div>

    <script src="http://d3js.org/d3.v3.min.js"></script>
    <script src="static/d3.layout.cloud.js"></script>
    <script src="word_cloud.js"></script>

</body>
</html>

